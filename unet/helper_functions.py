# AUTHOR: JT MAHONEY
# DATE: 08FEB2019

import numpy as np
from helper_fxns.pixelshift37 import *
from helper_fxns.roi import ROIEncoder, ROIRect, ROIDecoder
import tifffile
from skimage.feature import peak_local_max
from skimage.draw import polygon_perimeter
from skimage import io
from sklearn.cluster import DBSCAN
from bokeh.plotting import figure, show
from bokeh.io import output_notebook
from bokeh.palettes import Category20 as palette
import itertools
import collections
import pandas as pd

import mahotas as mh
from skimage.filters import threshold_otsu
from skimage.transform import downscale_local_mean

import os
import shutil
import glob
import zipfile

import json
from PIL import Image

from unet.mahoney import split_2d_img, all_the_kings_men
from keras.preprocessing.image import img_to_array, load_img
from unet import utils



def generate_json(channel_dictionary, dichroic_dictionary, output_directory):
# INPUT:
#   ch_dictionary:
#   file_name:
#   ch_colors:
    save_path = os.path.join(output_directory, 'image_data.json')

    img_data = {
        "channel_dictionary":channel_dictionary,
        "dichroic_dictionary":dichroic_dictionary
    }

    with open(save_path, 'w+') as write_file:
        json.dump(img_data, write_file)


def read_json(data_file):
    with open(data_file, "r") as read_file:
        data = json.load(read_file)
        return(data)



def shrink_image(multi_ch_img, ch_dict, shrink_scale):
    image_stack = []
    for key, value in ch_dict.items():
        img = Image.fromarray(multi_ch_img[key,:,:])
        h = int(multi_ch_img[key,:,:].shape[0]/shrink_scale)
        w = int(multi_ch_img[key,:,:].shape[1]/shrink_scale)
        img = img.resize((w,h), Image.ANTIALIAS)
        np_img = np.array(img)
        image_stack.append(np_img)
    shrunk_img = np.dstack(image_stack)
    shrunk_img = np.moveaxis(shrunk_img, -1, 0)
    return(shrunk_img)
    


def shrink_roi_labels(dimension_list,shrink_scale):
    loader_list = []
    for n in dimension_list:
        nn = int(n/shrink_scale)
        loader_list.append(nn)
    return(loader_list)

    


def save_png(multi_ch_img, ch_dict, destination_folder):
    for key, value in ch_dict.items():
        img = Image.fromarray(multi_ch_img[key])
        save_file = value + '_max-z.png'
        save_file = os.path.join(destination_folder, save_file)
        img.save(save_file)


def save_png_with_rois(multi_ch_img, ch_dict, destination_folder, df_all_rois):
    for key, value in ch_dict.items():
        img = multi_ch_img[key]
        save_file = value + '_labeled.png'
        save_file = os.path.join(destination_folder, save_file)
        
        single_channel_df = df_all_rois.loc[df_all_rois['channel_name']==value]
        for index, col in single_channel_df.iterrows():
            x1 = col['x1']
            y1 = col['y1']
            x2 = col['x2']
            y2 = col['y2']
            x_coords = [x1, x1, x2, x2]
            y_coords = [y1, y2, y2, y1]

            rr, cc = polygon_perimeter(y_coords, x_coords, shape=img.shape)
            try:
                
                img[rr,cc]=np.max(img)
            except:
                print("ERROR DRAWING RECTANGLE AT: ", rr, cc)
        io.imsave(save_file, img)




def convert_16_to_8(np_img):
# INPUT:
#     np_img: numpy image.  shape = 2D or 3D

# OUTPUT:
#     return: 8 bit version of origional np_img

    info = np.iinfo(np_img.dtype)
    if np.issubdtype(np_img.dtype, np.dtype('uint16')):
        data = np_img.astype(np.int16)/4095
        # print('ORIGIONAL IMG: ', np.max(np_img))
        # print('CONVERSION: ', np.max(data), " INFO: ", info)
        data = 255 * data
        img8 = data.astype(np.uint8)
        return(img8)
    elif np.issubdtype(np_img.dtype, np.dtype('uint8')):
        return(np_img)




def pixel_shift_3d(np_img, dichroic_dictionary, json_file):
# INPUT: 
#     np_img: numpy image. shape = 3D
#     dichroic_dictionary: dictionary where key=channel number, value=name of dichroic_dictionary
#     json_file: path to the .json file containing the shift values for each dichroic generated by the shift callibration script

# OUTPUT:
#     return: numpy image of shape (n_channels, height, width).  height and width may be different than origional image

    number_of_channels = np_img.shape[0]
    shifted_list = []

    # instantiate PixelShifter class object from pixelshift37
    shift_obj = PixelShifter(jsonfilepath=json_file)

    # perform shift on each channel
    for ch in range(np_img.shape[0]):
        ch_img = np_img[ch,:,:]
        ch_dichroic = dichroic_dictionary[ch]
        for key in shift_obj.shiftdict.keys():
            if key.split('_')[-1].lower() in ch_dichroic.lower():
                ch_img = shift_obj.shape_images(ch_img)
                shiftval = shift_obj.shiftdict[key]
        img = shift_obj.align_with_shift_matrix(ch_img,shiftval)
        shifted_list.append(img)
    
    # create numpy array where .shape = 3 from list of lists 'shifted_list'
    shifted_img = np.dstack(shifted_list)

    # rearranges shape of numpy array to look more like origional image
    shifted_img = np.moveaxis(shifted_img, -1, 0)
    print("shifted img shape: ", shifted_img.shape)
    
    return(shifted_img)


def correct_shift_upsampling(img):
    if np.issubdtype(img.dtype, np.dtype('uint16')):
        over_values = img > 4095
        img[over_values] = 4095
        return(img)
    elif np.issubdtype(img.dtype, np.dtype('uint8')):
        over_values = img > 255
        img[over_values] = 255
        return(img)


def ml_model_segment(single_channel_array, model, orig_x_dim=2048, orig_y_dim=2048, diff_out_size=True, mask_threshold=0.5, mask_min_pixel=300):
    img = single_channel_array
    img = img_to_array(img)
    img /= 255
    
    pv_model = model

    img_slices = split_2d_img(img, x_dim=orig_x_dim, y_dim=orig_x_dim, diff_out_size=diff_out_size) # gets converted from np.shape = 3 to np.shape = 4
    
    predicted_img_slices_list = []
    for s in img_slices:
        pred_new = pv_model.predict(np.asarray([s['slice_img']]))
        pred_new_list = [pred_new[i] for i in range(pred_new.shape[0])]
        # pred_new_mask = utils.label_mask(pred_new_list[0])
        s['mask'] = pred_new_list[0]
        predicted_img_slices_list.append(s)

    p_img, p_mask = all_the_kings_men(predicted_img_slices_list, img.shape[1], img.shape[0])
    lab_mask = utils.label_mask(p_mask, threshold=mask_threshold, min_pixel=mask_min_pixel)
    
    print('LAB MASK SHAPE: ', lab_mask.shape)

    region_props=regionprops(lab_mask,intensity_image=p_img)
    model_bb_array = bbs_from_rprops(region_props)
    return(model_bb_array, lab_mask)



def dot_segment(single_channel_array, threshold=40, max_peaks=6000, min_samples_for_cluster=15):
# INPUT:
#     single_channel_array: numpy image of single channel
#     threshold: minimum value of threshold for subtracting background.  anything less than this value is given a value of 0
#     max_peaks: maximum number of dots to be counted in a single channel image
#     min_samples_for_cluster: minimum number of peaks required to count as a cell when clustered with DBSCAN
# OUTPUT:
#     bb_array: a numpy array for the coordinates of each bounding box
#     dot_coords: coordinates of each peak detected (mostly for troubleshooting.  value usually dropped ,_)

    dot_img = single_channel_array

    # get coordinates of peaks/dots
    dot_coords = peak_local_max(dot_img, min_distance=2, threshold_abs=threshold, num_peaks=max_peaks)
    X = np.column_stack([dot_coords[:,0], dot_coords[:,1]])

    # generate clusters from regions of dots with adiquate densities
    db = DBSCAN(eps=25., min_samples=min_samples_for_cluster).fit(dot_coords)

    # generate boolean masks and labels from db clusters
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_

    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    counter = collections.Counter(labels)
    
    # create array and fill with coordinates of cluster bounding boxes
    # colors not used at the moment.  used to be used to generate a cluster graph where each cluseter was a different color
    labels_reshape = labels.reshape(labels.shape[0], 1)
    labeled_coords = np.hstack((dot_coords, labels_reshape))
    unique_labels = set(labels)
    colors = itertools.cycle(palette[10])
    bb_array = np.empty((0,5), dtype=int)
    
    for k, col in zip(unique_labels, colors):
        if k == -1:
            col = [0,0,0,1]
        
        class_member_mask = (labels == k)
        xy = X[class_member_mask & core_samples_mask]
        if xy.shape[0] != 0:
            x_min = np.amin(xy[:,1])
            x_max = np.amax(xy[:,1])
            y_min = np.amin(xy[:,0])
            y_max = np.amax(xy[:,0])
            
            k_list = [k, x_min, x_max, y_min, y_max]
            bb_array = np.append(bb_array, [k_list], axis=0)

    return(bb_array, dot_coords)




def watershed_single_channel(np_array, threshold=10, blur_factor=9, max_bb_size=13000, min_bb_size=1000, footprint=10):
# INPUT:
#     np_array: numpy image of single channel
#     threshold: minimum value to be analysed
#     blur_factor: gaussian blur.  some blur is good.  too much is bad.  too little is bad too.
#     max_bb_size: set maximum size for a bounding box.
#     min_bb_size: set minimum size for a bounding box.
#     footprint: box of size footprint x footprint used to find regions of maximum intensity.
# OUTPUT:
#     return: array of bounding boxes for the channel image given
# NOTE: input values should be tuned.  this alg is useless if input arguments are not optimized... right now its by hand.  otherwise just leave them

    # nuclear is a blured version of origional image
    nuclear = mh.gaussian_filter(np_array, blur_factor)
    # calculate a minimum threshold using otsu method
    otsu_thresh = threshold_otsu(nuclear)
    
    # determine minimum threshold from otsu and input argument.  if little/no signal in image; otsu value can be way too low
    set_thresh = None
    if threshold>otsu_thresh:
        set_thresh = threshold
    else:
        set_thresh = otsu_thresh
    
    # set values lower than set_thresh to zero
    index_otsu = nuclear < set_thresh
    nuclear[index_otsu] = 0
    
    # determine areas of maximum intensity and the distance between them
    thresh = (nuclear > nuclear.mean())
    dist = mh.stretch(mh.distance(thresh))
    Bc = np.ones((footprint, footprint))

    # the code the generate region_props from watersheding alg.
    maxima = mh.morph.regmax(dist, Bc=Bc)
    spots, n_spots = mh.label(maxima, Bc=Bc)
    sizes = mh.labeled.labeled_size(spots)
    too_big = np.where(sizes > max_bb_size)
    spots = mh.labeled.remove_regions(spots, too_big)
    spots = mh.labeled.remove_bordering(spots)
    spots, n_left = mh.labeled.relabel(spots)
    surface = (dist.max() - dist)
    areas = mh.cwatershed(surface, spots)
    areas *= thresh

    # get list of region properties from watershed.  allot of information in region_props.  allot of which is inaccurate.  NEVER TRUST REGIONPROPS!
    region_props=regionprops(areas,intensity_image=nuclear)

    # generate array of bounding boxes from measured region properties. call bbs_from_rprops()
    watershed_bb_array = bbs_from_rprops(region_props, max_bb_size, min_bb_size)

    return(watershed_bb_array)




def bbs_from_rprops(rprop_list, max_bb_size=10000, min_bb_size=1):
# INPUT:
#     rprop_list: origional region properties list. has allot of unecessary information
#     max_bb_size: maximum bounding box size allowed
#     min_bb_size: minimum bounding box size allowed
# OUTPUT:
#     return: array of bouding boxes generated with the watershedding alg.


    watershed_bb_list = []
    for cell_n in range(len(rprop_list)):
        rprops = rprop_list[cell_n]
        y1 = rprops['bbox'][0]
        x1 = rprops['bbox'][1]
        y2 = rprops['bbox'][2]
        x2 = rprops['bbox'][3]
        area = (abs(y2-y1)*abs(x2-x1))
        
        # gate to make sure bbs are the correct size
        if area > min_bb_size and area < max_bb_size:
            watershed_bb_list.append([cell_n,x1,x2,y1,y2])

    water_bbs = np.asarray(watershed_bb_list)
    return(water_bbs)



def make_roi_files(segmented_cells_csv, output_dir):
    df = pd.read_csv(segmented_cells_csv)
    unique_ch_names = df.channel_name.unique()


    for ch in unique_ch_names:
        ch_df = df[df.channel_name == ch]
        d_path = output_dir + '/' + ch + '_RoiSet/' 
        zip_url = output_dir + '/' + ch + '_RoiSet'
        
        if not os.path.exists(d_path):
            os.makedirs(d_path)
        
        for inx, row in ch_df.iterrows():
            x1 = row['x1']
            y1 = row['y1']
            x2 = row['x2']
            y2 = row['y2']

            f_string = d_path + str(inx) + '.roi'
            roi_obj = ROIRect(y1,x1,y2,x2)
            with ROIEncoder(f_string, roi_obj) as roi:
                roi.write()

        shutil.make_archive(zip_url, 'zip', d_path)
        shutil.rmtree(d_path)



# def read_imagej_roi_zip(filename):
    
#     roi_list = []
#     with zipfile.ZipFile(filename) as zf:
#         for name in zf.namelist():
#             roi_path = zf.extract(name, '/tmp')
#             roi = read_roi(roi_path)
#             if roi is None:
#                 continue

#             # label = str(name).rstrip('.roi')
#             # if dict_format:
#             #     roi_list.append({'label': label, 'polygons': roi.T})
#             # else:
#             #     roi_list.append([label, roi])
#             roi_list.append(roi)
#         return roi_list


# def read_roi(roi_path):
#     try:
#         with ROIDecoder(roi_path) as roi:
#             r = roi.get_roi()
#             #print(r.name)
#             roi_dictionary = {}
#             roi_dictionary['top'] = r.header['TOP']
#             roi_dictionary['bottom'] = r.header['BOTTOM']
#             roi_dictionary['left'] = r.header['LEFT']
#             roi_dictionary['right'] = r.header['RIGHT']
#             # r.x_coords = r.left + r.x_coords
#             # r.y_coords = r.top + r.y_coords
#             return(roi_dictionary)

#     except Exception as other:
#             print(roi_path, other)
#             return None

def read_imagej_roi_zip(filename):
    from roi_fxns import ijroi
    rois = ijroi.read_roi_zip(filename)
    roi_list = []
    for r in rois:
        # print(r)
        r = r[1]
        ys = r[:,0]
        xs = r[:,1]
        roi_dict = {}
        roi_dict['top'] = np.min(ys)
        roi_dict['bottom'] = np.max(ys)
        roi_dict['left'] = np.min(xs)
        roi_dict['right'] = np.max(xs)

        roi_list.append(roi_dict)
    return(roi_list)






def make_new_csv_from_rois(zip_file_list, coexpression_by_index_url, corr_count_url, segmented_cells_url):
# INPUT:
#     zip_file_list: list of zip files containing the regions of interest corrected using ImageJ/Fiji
# OUTPUT:
#     return: dictionary of channels (may be removed in the future.  seems unneccessary)
#     creates .csv file of correlations and new roi file

    # dataframe for all baounding box data for all channels of an image
    df_for_csv = pd.DataFrame()
    channel_dictionary = {}
    df = pd.read_csv(segmented_cells_url)
    
    # for each .zip file of rois
    for f in zip_file_list:
        
        # read each .zip file and get the name of the channel that file belongs to from the title
        rs = read_imagej_roi_zip(f)
        first_split = f.split('/')
        first_split = first_split[-1]
        split_list = first_split.split('_')
        ch_name = split_list[-2]
        
        # use channel name to find channel number from the segmented_cells.csv dataframe
        coresponding_ch_number = df[df.channel_name == str(ch_name)]
        coresponding_ch_number = coresponding_ch_number.channel_number.unique()
        # make sure channel name/number matching worked
        assert len(coresponding_ch_number) == 1
        ch_number = int(coresponding_ch_number)
        channel_dictionary[ch_number] = ch_name

        for r in rs:
            # read rois from dictionary
            x1 = r['left']
            y1 = r['top']
            x2 = r['right']
            y2 = r['bottom']
            
            # add bounding box data to the pandas dataframe
            df_dic = {"channel_number":ch_number,"channel_name":ch_name,"x1":x1,"x2":x2,"y1":y1,"y2":y2}
            df_for_csv = df_for_csv.append(df_dic, ignore_index=True)
    
    # convert numerical data from dtype string to dtype int
    df_for_csv[["channel_number","x1","x2","y1","y2"]] = df_for_csv[["channel_number","x1","x2","y1","y2"]].apply(pd.to_numeric)
    df_for_csv = df_for_csv.reset_index(drop=True)

    # find_correlations() calculates bb overlap and writes two .csv files
    corr_index_df = find_correlations(df_for_csv, channel_dictionary, coexpression_by_index_url, corr_count_url)
    return(corr_index_df)
    # return(channel_dictionary)


def iou(bb1, bb2):


    # determine the coordinates of the intersection rectangle
    x_left = max(bb1['x1'], bb2['x1'])
    y_top = max(bb1['y1'], bb2['y1'])
    x_right = min(bb1['x2'], bb2['x2'])
    y_bottom = min(bb1['y2'], bb2['y2'])

    if x_right <= x_left or y_bottom <= y_top:
        return 0.0

    ###Common fxn that i litterally stole from a dude on stack
    assert bb1['x1'] < bb1['x2']
    assert bb1['y1'] < bb1['y2']
    assert bb2['x1'] < bb2['x2']
    assert bb2['y1'] < bb2['y2']


    # The intersection of two axis-aligned bounding boxes is always an
    # axis-aligned bounding box
    intersection_area = (x_right - x_left) * (y_bottom - y_top)

    # compute the area of both AABBs
    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])
    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])

    # compute the intersection over union by taking the intersection
    # area and dividing it by the sum of prediction + ground-truth
    # areas - the interesection area
    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)
    # print(iou)
    assert iou >= 0.0
    assert iou <= 1.0
    return iou


def find_correlations(multi_ch_df, channel_dictionary, index_file_url, corr_count_url, thresh_iou_val=0.3):
    dict_of_ch=channel_dictionary
    
    # get channel numbers
    slice_nums = multi_ch_df.channel_number.unique()
    slice_nums.sort()
    
    # start df object where index and columns are the channel numbers.  Fill df with zeros
    corr_df = pd.DataFrame(index=slice_nums, columns=slice_nums)
    corr_df[:] = 0
    
    # I HAVE NO IDEA WHAT THIS IS FOR
    corr_index_df = multi_ch_df
    for ch_num_for_df in slice_nums:
        ch_name_for_df = dict_of_ch[ch_num_for_df]
        corr_index_df[ch_name_for_df] = None

    for ch_1 in slice_nums:
        # get values from df for a sepecific channel number
        df_1 = multi_ch_df.loc[multi_ch_df['channel_number'] == ch_1]
        
        # iterate through bbs for channel 1
        for index1, row1 in df_1.iterrows():
            df1_y1 = row1['y1']
            df1_x1 = row1['x1']
            df1_y2 = row1['y2']
            df1_x2 = row1['x2']
            bb1 = {'x1':df1_x1, 'x2':df1_x2, 'y1':df1_y1, 'y2':df1_y2}
            for ch_2 in slice_nums:
                # get data for a second channel
                df_2 = multi_ch_df.loc[multi_ch_df['channel_number'] == ch_2]
                # iterate through bbs for channel 2
                for index2, row2 in df_2.iterrows():
                    df2_y1 = row2['y1']
                    df2_x1 = row2['x1']
                    df2_y2 = row2['y2']
                    df2_x2 = row2['x2']
                    bb2 = {'x1':df2_x1, 'x2':df2_x2, 'y1':df2_y1, 'y2':df2_y2}
                    
                    # does that bb from channel 1 and 2 overlap?  iou() returns a value between 0 and 1 for overlap
                    iou_val = iou(bb1, bb2)
                    # is the overlap greater than the threshold for overlap?
                    if iou_val > thresh_iou_val:
                        # if bbs overlap, add 1 to the correlation df
                        add_df_val = corr_df.at[ch_1, ch_2] + 1
                        corr_df.at[ch_1, ch_2] = add_df_val
                            
                        # add to the co-expression dataframe.  a df where all the bbs are listed by index, and each bb
                        # has a row indicating the gene it overlaps with and the index of the specific bb it overlaps with
                        co_expressor_name = dict_of_ch[ch_2]
                        co_expressor_prev_val = corr_index_df.at[index1, co_expressor_name]
                        if co_expressor_prev_val == None:
                            index_list = [index2]
                            corr_index_df.at[index1, co_expressor_name]=index_list
                        else:
                            co_expressor_prev_val.append(index2)
                            corr_index_df.at[index1, co_expressor_name]=co_expressor_prev_val

    # replace channel number with channel names in the corr_df dataframe
    corr_df = corr_df.rename(columns=dict_of_ch, index=dict_of_ch)

    # print(corr_index_df)           
    print(corr_df)
    
    # save the corr_df and corr_index_df as .csv
    corr_index_df.to_csv(index_file_url, encoding='utf-8', index=True)
    corr_df.to_csv(corr_count_url, encoding='utf-8', index=True)
    return(corr_index_df)



def show_bbs(bb_df):
    # INPUT:
    #     bb_df: dataframe of all bbs
    # OUTPUT: bokeh figure for a channel
    # this is just for quick refference of a channel.  Not used normally
    df = bb_df[bb_df.channel_number==4]
    #print(df)
    top = df['y1'].tolist()
    bottom = df['y2'].tolist()
    left = df['x1'].tolist()
    right = df['x2'].tolist()
    
    neg_top = [-x for x in top]
    neg_bottom = [-x for x in bottom]

    p = figure()
    p.quad(left=left, right=right, top=neg_top, bottom=neg_bottom)
    show(p)
    
